{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "intensive-butter",
      "metadata": {
        "id": "intensive-butter"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True)\n",
        "# or\n",
        "# model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet101', pretrained=True)\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from generate_training_validation_data import CustomImageDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "train_data_dir = 'D:/MemeMachine_ProjectData/dataset/training'\n",
        "validation_data_dir = 'D:/MemeMachine_ProjectData/dataset/validation'\n",
        "img_width, img_height, n_channels = 128, 128, 3 #TODO change dimensions to be wider, to better support text\n",
        "\n",
        "epochs = 1 #50 TODO\n",
        "batch_size = 1\n",
        "classes = ['nothing', 'text']\n",
        "# classes = ['text']\n",
        "\n",
        "\n",
        "#change the number of classes in the final step of the classifier\n",
        "# print(model.classifier[4])\n",
        "model.classifier[4] = torch.nn.Conv2d(512, len(classes), kernel_size=(1,1), stride = (1,1))\n",
        "torch.nn.init.xavier_uniform(model.classifier[4].weight)\n",
        "\n",
        "# print(model.aux_classifier[4])\n",
        "model.aux_classifier[4] = torch.nn.Conv2d(256, len(classes), kernel_size=(1,1), stride = (1,1))\n",
        "torch.nn.init.xavier_uniform(model.aux_classifier[4].weight)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0c9113ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_name = \"useBothClassesWithSoftmaxAndSubtract\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "066cdf42",
      "metadata": {},
      "outputs": [],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "%env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "import torch\n",
        "torch.backends.cuda.matmul.allow_tf32 = True\n",
        "torch.backends.cudnn.benchmark = True\n",
        "torch.backends.cudnn.deterministic = False\n",
        "torch.backends.cudnn.allow_tf32 = True\n",
        "data = torch.randn([1, 512, 16, 16], dtype=torch.float, device='cuda', requires_grad=True)\n",
        "net = torch.nn.Conv2d(512, 1, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\n",
        "net = net.cuda().float()\n",
        "out = net(data)\n",
        "out.backward(torch.randn_like(out))\n",
        "torch.cuda.synchronize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-belize",
      "metadata": {
        "id": "accredited-belize"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(N, 3, H, W)`, where `N` is the number of images, `H` and `W` are expected to be at least `224` pixels.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "The model returns an `OrderedDict` with two Tensors that are of the same height and width as the input Tensor, but with 21 classes.\n",
        "`output['out']` contains the semantic masks, and `output['aux']` contains the auxillary loss values per-pixel. In inference mode, `output['aux']` is not useful.\n",
        "So, `output['out']` is of shape `(N, 21, H, W)`. More documentation can be found [here](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "thirty-crown",
      "metadata": {
        "id": "thirty-crown"
      },
      "outputs": [],
      "source": [
        "# Select an image from the dataset\n",
        "\n",
        "#TODO change image_with_text_functions.generate_text_on_image_and_pixel_mask_from_path to place the text properly\n",
        "train_dataset = CustomImageDataset(train_data_dir, img_width, img_height)\n",
        "test_dataset = CustomImageDataset(validation_data_dir, img_width, img_height)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, )\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Display image and label.\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "input_image = train_features[0].squeeze()\n",
        "input_image = np.moveaxis(input_image.numpy(), 0, -1)\n",
        "label = train_labels[0].reshape((img_width, img_height))\n",
        "\n",
        "# print('train_labels[0]', train_labels[0].max())\n",
        "# print('label', label.max())\n",
        "# label = label*256\n",
        "# label = label.long()\n",
        "# print('label', label.max())\n",
        "# print(train_labels[0])\n",
        "# print(label.shape)\n",
        "\n",
        "\n",
        "plt.imshow(input_image, cmap=\"gray\")\n",
        "plt.show()\n",
        "plt.imshow(label, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "running-blogger",
      "metadata": {
        "id": "running-blogger"
      },
      "outputs": [],
      "source": [
        "# sample execution (requires torchvision)\n",
        "from PIL import Image\n",
        "from torchvision import transforms\n",
        "import cv2 as cv\n",
        "# input_image = Image.open(filename)\n",
        "input_image2 = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
        "input_image2 = Image.fromarray(np.uint8(input_image2))\n",
        "input_image2 = input_image2.convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "print(input_batch.shape)\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0] #zero refers to the batch number?\n",
        "output_predictions = output.argmax(0)\n",
        "# print(output_predictions)\n",
        "# print(output_predictions.shape)\n",
        "# print(output)\n",
        "# print(output.shape)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesbian-vietnamese",
      "metadata": {
        "id": "lesbian-vietnamese"
      },
      "source": [
        "The output here is of shape `(21, H, W)`, and at each location, there are unnormalized probabilities corresponding to the prediction of each class.\n",
        "To get the maximum prediction of each class, and then use it for a downstream task, you can do `output_predictions = output.argmax(0)`.\n",
        "\n",
        "Here's a small snippet that plots the predictions, with each color being assigned to each class (see the visualized image on the left)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "starting-delhi",
      "metadata": {
        "id": "starting-delhi"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "FCN-ResNet is constructed by a Fully-Convolutional Network model, using a ResNet-50 or a ResNet-101 backbone.\n",
        "The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
        "\n",
        "Their accuracies of the pre-trained models evaluated on COCO val2017 dataset are listed below.\n",
        "\n",
        "| Model structure |   Mean IOU  | Global Pixelwise Accuracy |\n",
        "| --------------- | ----------- | --------------------------|\n",
        "|  fcn_resnet50   |   60.5      |   91.4                    |\n",
        "|  fcn_resnet101  |   63.7      |   91.9                    |\n",
        "\n",
        "### Resources\n",
        "\n",
        " - [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "193ad198",
      "metadata": {},
      "outputs": [],
      "source": [
        "from collections import namedtuple\n",
        "from typing import Any\n",
        "import torch\n",
        "\n",
        "\n",
        "# pylint: disable = abstract-method\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper class for model with dict/list rvalues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: torch.nn.Module) -> None:\n",
        "        \"\"\"\n",
        "        Init call.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_x: torch.Tensor) -> Any:\n",
        "        \"\"\"\n",
        "        Wrap forward call.\n",
        "        \"\"\"\n",
        "        data = self.model(input_x)\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            data_named_tuple = namedtuple(\"ModelEndpoints\", sorted(data.keys()))  # type: ignore\n",
        "            data = data_named_tuple(**data)  # type: ignore\n",
        "\n",
        "        elif isinstance(data, list):\n",
        "            data = tuple(data)\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "\n",
        "# default `log_dir` is \"runs\" - we'll be more specific here\n",
        "writer = SummaryWriter('runs/FCN_resnet101_GPU_text_pixel_masking/'+experiment_name) \n",
        "\n",
        "# get some random training images\n",
        "dataiter = iter(train_dataloader)\n",
        "images, labels = dataiter.next()\n",
        "\n",
        "# create grid of images\n",
        "img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# write to tensorboard\n",
        "writer.add_image('FCN_resnet101_GPU_text_pixel_masking_images', img_grid)\n",
        "\n",
        "model_wrapper = ModelWrapper(model)\n",
        "writer.add_graph(model_wrapper, torch.tensor(images).to('cuda'))\n",
        "writer.close()\n",
        "\n",
        "\n",
        "def plot_classes_preds(images, preds, labels):\n",
        "    '''\n",
        "    Generates matplotlib Figure using a trained network, along with images\n",
        "    and labels from a batch, that shows the network's top prediction along\n",
        "    with its probability, alongside the actual label, coloring this\n",
        "    information based on whether the prediction was correct or not.\n",
        "    Uses the \"images_to_probs\" function.\n",
        "    '''\n",
        "\n",
        "    fig = plt.figure(figsize=(8, 4))\n",
        "\n",
        "    fig.add_subplot(1, 3, 1, xticks=[], yticks=[])\n",
        "    input_image = images.squeeze()\n",
        "    # print('input_image', input_image.shape)\n",
        "    plt.imshow(input_image)\n",
        "\n",
        "    fig.add_subplot(1, 3, 2, xticks=[], yticks=[])\n",
        "    plt.imshow(preds.reshape((img_height,img_width)).detach().cpu().numpy(), cmap='gray')\n",
        "\n",
        "    fig.add_subplot(1, 3, 3, xticks=[], yticks=[])\n",
        "    label = labels.reshape((img_width, img_height)).long().detach().cpu()\n",
        "    plt.imshow(label, cmap='gray')\n",
        "\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f449bec8",
      "metadata": {},
      "outputs": [],
      "source": [
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "# criterion = nn.CrossEntropyLoss(size_average=False, reduction='sum')\n",
        "criterion = nn.SoftMarginLoss(size_average=False, reduction='sum')\n",
        "# optimizer = optim.SGD(model.parameters(), lr=10**-10, momentum=0.99) #TODO change learning rate to a lower number\n",
        "if torch.cuda.is_available():\n",
        "    model.to('cuda')\n",
        "optimizer = optim.Adam(model.parameters())\n",
        "model.train()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7e2c12f",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.train()\n",
        "\n",
        "# Train the model\n",
        "mini_epoch_loss = 0.0\n",
        "epoch_loss = 0.0\n",
        "for epoch in range(100):  # loop over the dataset multiple times  \n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        # get the inputs; data is a list of [inputs, labels]\n",
        "        inputs, labels = data\n",
        "        # print(inputs.shape)\n",
        "\n",
        "        inputs = inputs.squeeze()\n",
        "        inputs = np.moveaxis(inputs.detach().cpu().numpy(), 0, -1)\n",
        "        # print(inputs.shape)\n",
        "        preprocessed_inputs = cv.cvtColor(inputs, cv.COLOR_BGR2RGB)\n",
        "        preprocessed_inputs = Image.fromarray(np.uint8(preprocessed_inputs))\n",
        "        preprocessed_inputs = preprocessed_inputs.convert(\"RGB\")\n",
        "        preprocessed_inputs = preprocess(preprocessed_inputs)\n",
        "        preprocessed_inputs = preprocessed_inputs.unsqueeze(0)\n",
        "\n",
        "        # move the input and model to GPU for speed if available\n",
        "        if torch.cuda.is_available():\n",
        "            preprocessed_inputs = preprocessed_inputs.to('cuda')\n",
        "            labels = labels.to('cuda')\n",
        "            model.to('cuda')\n",
        "            \n",
        "\n",
        "        # zero the parameter gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        # forward + backward + optimize\n",
        "        output = model(preprocessed_inputs)['out'][0]\n",
        "        # print(output.shape)\n",
        "        output = nn.Softmax(dim=0)(output)\n",
        "        output = output[1] - output[0]\n",
        "        # print(output.shape)\n",
        "        # print(output)\n",
        "        # print(output.shape)\n",
        "        output = torch.unsqueeze(output, 0)\n",
        "        output = torch.unsqueeze(output, 0)\n",
        "        # output = torch.sigmoid(output) FIXME why use this?\n",
        "        # print(output.shape)\n",
        "\n",
        "        # output_predictions = output.argmax(0)\n",
        "        labels = torch.reshape(labels, (1, img_height,img_width))\n",
        "        # labels = labels*256\n",
        "        labels[labels<=0] = -1\n",
        "        labels[labels>0] = 1\n",
        "        labels = labels.long()\n",
        "        # print(\"output\", output)\n",
        "\n",
        "\n",
        "        loss = criterion(output, labels)\n",
        "        \n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        # print statistics\n",
        "        epoch_loss += loss.item()\n",
        "        mini_epoch_loss += loss.item()\n",
        "        if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "        # if i % 100 == 1:    # print every 100 mini-batches\n",
        "        # if i % 2 == 1:    # print every 2 mini-batches\n",
        "            print(i)\n",
        "            print(\"cost\", loss)\n",
        "            writer.add_scalar('total mini-epoch loss',\n",
        "                mini_epoch_loss,\n",
        "                epoch * len(train_dataloader) + i)\n",
        "                \n",
        "            writer.add_scalar('average mini-epoch loss',\n",
        "                mini_epoch_loss / 1000,\n",
        "                epoch * len(train_dataloader) + i)\n",
        "\n",
        "            mini_epoch_loss = 0.0\n",
        "            writer.add_scalar('most recent cost',\n",
        "                            loss.item(),\n",
        "                            epoch * len(train_dataloader) + i)\n",
        "\n",
        "            # ...log a Matplotlib Figure showing the model's predictions on a\n",
        "            # random mini-batch\n",
        "            writer.add_figure('input vs output vs label',\n",
        "                            plot_classes_preds(inputs, output, labels),\n",
        "                            global_step=epoch * len(train_dataloader) + i)\n",
        "  \n",
        "    writer.add_scalar('total epoch loss',\n",
        "                    epoch_loss,\n",
        "                    epoch * len(train_dataloader) + i)\n",
        "    writer.add_scalar('average epoch loss',\n",
        "                    epoch_loss / \n",
        "                    (epoch * len(train_dataloader) + i),\n",
        "                    epoch * len(train_dataloader) + i)\n",
        "    torch.save({\n",
        "            'epoch': epoch,\n",
        "            'model_state_dict': model.state_dict(),\n",
        "            'optimizer_state_dict': optimizer.state_dict(),\n",
        "            'loss': loss,\n",
        "            }, 'model_saves/'+experiment_name)\n",
        " \n",
        "print('Finished Training')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79758fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8caa42c6",
      "metadata": {},
      "source": [
        "Show a test of the newly trained (fine tuned) model below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Display image and label.\n",
        "test_features, test_labels = next(iter(test_dataloader))\n",
        "print(f\"Feature batch shape: {test_features.size()}\")\n",
        "print(f\"Labels batch shape: {test_labels.size()}\")\n",
        "input_image = test_features[0].squeeze()\n",
        "input_image = np.moveaxis(input_image.numpy(), 0, -1)\n",
        "label = test_labels[0].reshape((img_width, img_height))\n",
        "\n",
        "plt.imshow(input_image, cmap=\"gray\")\n",
        "plt.show()\n",
        "plt.imshow(label, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f76b07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_image = Image.open(filename)\n",
        "input_image2 = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
        "input_image2 = Image.fromarray(np.uint8(input_image2))\n",
        "input_image2 = input_image2.convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0] #zero refers to the batch number?\n",
        "output_predictions = output.argmax(0)\n",
        "print(output_predictions)\n",
        "print(output_predictions.shape)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9d37c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "test(test_dataloader, model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee44acb2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_vision_fcn_resnet101.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
