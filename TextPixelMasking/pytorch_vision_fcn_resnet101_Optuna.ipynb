{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 8,
      "id": "intensive-butter",
      "metadata": {
        "id": "intensive-butter"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in C:\\Users\\maxan/.cache\\torch\\hub\\pytorch_vision_v0.10.0\n",
            "c:\\Users\\maxan\\Documents\\Programming\\MemeMachine\\MemeMachine\\TextPixelMasking\\GPUtextPixelMasking\\lib\\site-packages\\ipykernel_launcher.py:33: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n",
            "c:\\Users\\maxan\\Documents\\Programming\\MemeMachine\\MemeMachine\\TextPixelMasking\\GPUtextPixelMasking\\lib\\site-packages\\ipykernel_launcher.py:37: UserWarning: nn.init.xavier_uniform is now deprecated in favor of nn.init.xavier_uniform_.\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "FCN(\n",
              "  (backbone): IntermediateLayerGetter(\n",
              "    (conv1): Conv2d(3, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
              "    (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (relu): ReLU(inplace=True)\n",
              "    (maxpool): MaxPool2d(kernel_size=3, stride=2, padding=1, dilation=1, ceil_mode=False)\n",
              "    (layer1): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(64, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(256, 64, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(64, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(64, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer2): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(256, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(2, 2), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(256, 512, kernel_size=(1, 1), stride=(2, 2), bias=False)\n",
              "          (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(512, 128, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(128, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(128, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer3): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(512, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(512, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (3): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (4): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (5): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 256, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(256, 1024, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(1024, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "    (layer4): Sequential(\n",
              "      (0): Bottleneck(\n",
              "        (conv1): Conv2d(1024, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(2, 2), dilation=(2, 2), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "        (downsample): Sequential(\n",
              "          (0): Conv2d(1024, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "          (1): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        )\n",
              "      )\n",
              "      (1): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "      (2): Bottleneck(\n",
              "        (conv1): Conv2d(2048, 512, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv2): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(4, 4), dilation=(4, 4), bias=False)\n",
              "        (bn2): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (conv3): Conv2d(512, 2048, kernel_size=(1, 1), stride=(1, 1), bias=False)\n",
              "        (bn3): BatchNorm2d(2048, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "        (relu): ReLU(inplace=True)\n",
              "      )\n",
              "    )\n",
              "  )\n",
              "  (classifier): FCNHead(\n",
              "    (0): Conv2d(2048, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(512, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(512, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              "  (aux_classifier): FCNHead(\n",
              "    (0): Conv2d(1024, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1), bias=False)\n",
              "    (1): BatchNorm2d(256, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): Dropout(p=0.1, inplace=False)\n",
              "    (4): Conv2d(256, 2, kernel_size=(1, 1), stride=(1, 1))\n",
              "  )\n",
              ")"
            ]
          },
          "execution_count": 8,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "model = torch.hub.load('pytorch/vision:v0.10.0', 'fcn_resnet50', pretrained=True, )\n",
        "\n",
        "from torch.utils.data import DataLoader\n",
        "from generate_training_validation_data import CustomImageDataset\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "from PIL import Image\n",
        "import cv2 as cv\n",
        "\n",
        "from collections import namedtuple\n",
        "from typing import Any\n",
        "import torch\n",
        "import torch.optim as optim\n",
        "import torch.nn as nn\n",
        "\n",
        "train_data_dir = 'D:/MemeMachine_ProjectData/dataset/training'\n",
        "validation_data_dir = 'D:/MemeMachine_ProjectData/dataset/validation'\n",
        "img_width, img_height, n_channels = 1020, 1020, 3 #TODO change dimensions to be wider, to better support text\n",
        "\n",
        "epochs = 1 #50 TODO\n",
        "batch_size = 1\n",
        "classes = ['nothing', 'text']\n",
        "# classes = ['text']\n",
        "\n",
        "\n",
        "#change the number of classes in the final step of the classifier\n",
        "# print(model.classifier[4])\n",
        "model.classifier[4] = torch.nn.Conv2d(512, len(classes), kernel_size=(1,1), stride = (1,1))\n",
        "torch.nn.init.xavier_uniform(model.classifier[4].weight)\n",
        "\n",
        "# print(model.aux_classifier[4])\n",
        "model.aux_classifier[4] = torch.nn.Conv2d(256, len(classes), kernel_size=(1,1), stride = (1,1))\n",
        "torch.nn.init.xavier_uniform(model.aux_classifier[4].weight)\n",
        "\n",
        "model.eval()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "0c9113ab",
      "metadata": {},
      "outputs": [],
      "source": [
        "experiment_name = \"HugeText_increasedResolution_Test28\" "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "066cdf42",
      "metadata": {},
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'             CUDA Troubleshooting            '"
            ]
          },
          "execution_count": 10,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.backends.cudnn.benchmark = True\n",
        "\n",
        "\n",
        "\"\"\"             CUDA Troubleshooting            \"\"\"\n",
        "\n",
        "# %env CUDA_LAUNCH_BLOCKING=1\n",
        "\n",
        "# import torch\n",
        "# torch.backends.cuda.matmul.allow_tf32 = True\n",
        "# torch.backends.cudnn.benchmark = True\n",
        "# torch.backends.cudnn.deterministic = False\n",
        "# torch.backends.cudnn.allow_tf32 = True\n",
        "# data = torch.randn([1, 512, 16, 16], dtype=torch.float, device='cuda', requires_grad=True)\n",
        "# net = torch.nn.Conv2d(512, 1, kernel_size=[1, 1], padding=[0, 0], stride=[1, 1], dilation=[1, 1], groups=1)\n",
        "# net = net.cuda().float()\n",
        "# out = net(data)\n",
        "# out.backward(torch.randn_like(out))\n",
        "# torch.cuda.synchronize()\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "accredited-belize",
      "metadata": {
        "id": "accredited-belize"
      },
      "source": [
        "All pre-trained models expect input images normalized in the same way,\n",
        "i.e. mini-batches of 3-channel RGB images of shape `(N, 3, H, W)`, where `N` is the number of images, `H` and `W` are expected to be at least `224` pixels.\n",
        "The images have to be loaded in to a range of `[0, 1]` and then normalized using `mean = [0.485, 0.456, 0.406]`\n",
        "and `std = [0.229, 0.224, 0.225]`.\n",
        "\n",
        "The model returns an `OrderedDict` with two Tensors that are of the same height and width as the input Tensor, but with 21 classes.\n",
        "`output['out']` contains the semantic masks, and `output['aux']` contains the auxillary loss values per-pixel. In inference mode, `output['aux']` is not useful.\n",
        "So, `output['out']` is of shape `(N, 21, H, W)`. More documentation can be found [here](https://pytorch.org/vision/stable/models.html#object-detection-instance-segmentation-and-person-keypoint-detection)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "thirty-crown",
      "metadata": {
        "id": "thirty-crown"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Feature batch shape: torch.Size([1, 3, 1020, 1020])\n",
            "Labels batch shape: torch.Size([1, 1040400])\n"
          ]
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        },
        {
          "data": {
            "image/png": "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",
            "text/plain": [
              "<Figure size 432x288 with 1 Axes>"
            ]
          },
          "metadata": {
            "needs_background": "light"
          },
          "output_type": "display_data"
        }
      ],
      "source": [
        "import optuna\n",
        "\n",
        "# Select an image from the dataset\n",
        "\n",
        "#TODO change image_with_text_functions.generate_text_on_image_and_pixel_mask_from_path to place the text properly\n",
        "train_dataset = CustomImageDataset(train_data_dir, img_width, img_height)\n",
        "test_dataset = CustomImageDataset(validation_data_dir, img_width, img_height)\n",
        "\n",
        "train_dataloader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "test_dataloader = DataLoader(test_dataset, batch_size=batch_size, shuffle=True)\n",
        "\n",
        "# Display image and label\n",
        "train_features, train_labels = next(iter(train_dataloader))\n",
        "print(f\"Feature batch shape: {train_features.size()}\")\n",
        "print(f\"Labels batch shape: {train_labels.size()}\")\n",
        "input_image = train_features[0].squeeze()\n",
        "input_image = np.moveaxis(input_image.numpy(), 0, -1)\n",
        "label = train_labels[0].reshape((img_width, img_height))\n",
        "\n",
        "plt.imshow(input_image, cmap=\"gray\")\n",
        "plt.show()\n",
        "plt.imshow(label, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "lesbian-vietnamese",
      "metadata": {
        "id": "lesbian-vietnamese"
      },
      "source": [
        "The output here is of shape `(21, H, W)`, and at each location, there are unnormalized probabilities corresponding to the prediction of each class.\n",
        "To get the maximum prediction of each class, and then use it for a downstream task, you can do `output_predictions = output.argmax(0)`.\n",
        "\n",
        "Here's a small snippet that plots the predictions, with each color being assigned to each class (see the visualized image on the left)."
      ]
    },
    {
      "cell_type": "markdown",
      "id": "starting-delhi",
      "metadata": {
        "id": "starting-delhi"
      },
      "source": [
        "### Model Description\n",
        "\n",
        "FCN-ResNet is constructed by a Fully-Convolutional Network model, using a ResNet-50 or a ResNet-101 backbone.\n",
        "The pre-trained models have been trained on a subset of COCO train2017, on the 20 categories that are present in the Pascal VOC dataset.\n",
        "\n",
        "Their accuracies of the pre-trained models evaluated on COCO val2017 dataset are listed below.\n",
        "\n",
        "| Model structure |   Mean IOU  | Global Pixelwise Accuracy |\n",
        "| --------------- | ----------- | --------------------------|\n",
        "|  fcn_resnet50   |   60.5      |   91.4                    |\n",
        "|  fcn_resnet101  |   63.7      |   91.9                    |\n",
        "\n",
        "### Resources\n",
        "\n",
        " - [Fully Convolutional Networks for Semantic Segmentation](https://arxiv.org/abs/1605.06211)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "193ad198",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "# pylint: disable = abstract-method\n",
        "class ModelWrapper(torch.nn.Module):\n",
        "    \"\"\"\n",
        "    Wrapper class for model with dict/list rvalues.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, model: torch.nn.Module) -> None:\n",
        "        \"\"\"\n",
        "        Init call.\n",
        "        \"\"\"\n",
        "        super().__init__()\n",
        "        self.model = model\n",
        "\n",
        "    def forward(self, input_x: torch.Tensor) -> Any:\n",
        "        \"\"\"\n",
        "        Wrap forward call.\n",
        "        \"\"\"\n",
        "        data = self.model(input_x)\n",
        "\n",
        "        if isinstance(data, dict):\n",
        "            data_named_tuple = namedtuple(\"ModelEndpoints\", sorted(data.keys()))  # type: ignore\n",
        "            data = data_named_tuple(**data)  # type: ignore\n",
        "\n",
        "        elif isinstance(data, list):\n",
        "            data = tuple(data)\n",
        "\n",
        "        return data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "5986e6f8",
      "metadata": {},
      "outputs": [],
      "source": [
        "from torch.utils.tensorboard import SummaryWriter\n",
        "# # default `log_dir` is \"runs\" - we'll be more specific here\n",
        "# writer = SummaryWriter('runs/FCN_resnet101_GPU_text_pixel_masking/'+experiment_name) \n",
        "\n",
        "# # get some random training images\n",
        "# dataiter = iter(train_dataloader)\n",
        "# images, labels = dataiter.next()\n",
        "\n",
        "# # create grid of images\n",
        "# img_grid = torchvision.utils.make_grid(images)\n",
        "\n",
        "# # write to tensorboard\n",
        "# writer.add_image('FCN_resnet101_GPU_text_pixel_masking_images', img_grid)\n",
        "\n",
        "# model_wrapper = ModelWrapper(model)\n",
        "# writer = SummaryWriter('runs/FCN_resnet101_GPU_text_pixel_masking/'+experiment_name) \n",
        "\n",
        "# writer.add_graph(model_wrapper, torch.tensor(images))\n",
        "# writer.close()\n",
        "\n",
        "\n",
        "def plot_classes_preds(images, preds, labels):\n",
        "    '''\n",
        "    Generates matplotlib Figure using a trained network, along with images\n",
        "    and labels from a batch, that shows the network's top prediction along\n",
        "    with its probability, alongside the actual label, coloring this\n",
        "    information based on whether the prediction was correct or not.\n",
        "    Uses the \"images_to_probs\" function.\n",
        "    '''\n",
        "\n",
        "    fig = plt.figure(figsize=(30, 15))\n",
        "\n",
        "    fig.add_subplot(1, 3, 1, xticks=[], yticks=[])\n",
        "    input_image = images.squeeze()\n",
        "    # print('input_image', input_image.shape)\n",
        "    plt.imshow(input_image)\n",
        "\n",
        "    fig.add_subplot(1, 3, 2, xticks=[], yticks=[])\n",
        "    plt.imshow(preds.reshape((img_height,img_width)).detach().cpu().numpy(), cmap='gray')\n",
        "\n",
        "    fig.add_subplot(1, 3, 3, xticks=[], yticks=[])\n",
        "    label = labels.reshape((img_width, img_height)).long().detach().cpu()\n",
        "    plt.imshow(label, cmap='gray')\n",
        "\n",
        "\n",
        "    return fig"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "f449bec8",
      "metadata": {},
      "outputs": [],
      "source": [
        "\n",
        "\n",
        "#'model_saves/'+experiment_name+\".pytorch_model\"\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "fa46e836",
      "metadata": {},
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "c7e2c12f",
      "metadata": {},
      "outputs": [],
      "source": [
        "def objective(trial, model):\n",
        "    \n",
        "    writer = SummaryWriter('runs/FCN_resnet101_GPU_text_pixel_masking/'+experiment_name)#+\"/\"+str(trial.number)) \n",
        "\n",
        "    # criterion = nn.CrossEntropyLoss(size_average=False, reduction='sum')\n",
        "    criterion = nn.SoftMarginLoss(reduction='sum')\n",
        "    # criterion = nn.HingeEmbeddingLoss(margin=1.0, reduction='sum')\n",
        "    # optimizer = optim.SGD(model.parameters(), lr=10**-10, momentum=0.99) #TODO change learning rate to a lower number\n",
        "    if torch.cuda.is_available():\n",
        "        model.to('cuda')\n",
        "    \n",
        "    # optimizer = optim.Adam(model.parameters())\n",
        "    # Test 15 [I 2022-01-25 14:56:49,051] Trial 3 finished with value: 5567.529612823274 and parameters: {'lr': 6.630131189647407e-07, 'beta1': 0.006967112069996084, 'beta2': 0.9853718852556564}. Best is trial 3 with value: 5567.529612823274.\n",
        "    # lr = trial.suggest_loguniform('lr', 10**-8, 10**-4)\n",
        "    # beta1 = trial.suggest_loguniform('beta1', 1e-4, 1)\n",
        "    # beta2 = trial.suggest_loguniform('beta2', 0.8, 0.999)\n",
        "    # print(\"LR trial:\", lr)\n",
        "    # print(\"beta1 trial:\", beta1)\n",
        "    # print(\"beta2 trial:\", beta2)\n",
        "\n",
        "    optimizer = optim.Adam(model.parameters(), lr=6.630131189647407e-07, betas=(0.006967112069996084, 0.9853718852556564), eps=10**-7) # decided by 5 trials (epochs) of hyperparameter tuning with Optuna\n",
        "    scheduler = torch.optim.lr_scheduler.CyclicLR(optimizer, base_lr=10**-8, max_lr=10**-4, cycle_momentum=False)\n",
        "    \n",
        "    with open(\"model_saves\\\\best_model.txt\", 'r') as best_model_fh:\n",
        "            lines = best_model_fh.read().splitlines()\n",
        "            last_line = lines[-1]\n",
        "            best_model_path = last_line\n",
        "    if False:\n",
        "        path = best_model_path\n",
        "        checkpoint = torch.load(path)\n",
        "        model.load_state_dict(checkpoint['model_state_dict'])\n",
        "        optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "        epoch = checkpoint['epoch']\n",
        "        loss = checkpoint['loss']\n",
        "        print(\"successfully loaded model checkpoint from\", path)\n",
        "\n",
        "\n",
        "    model.train()\n",
        "\n",
        "    # Train the model\n",
        "    last_epoch_loss = 0.0\n",
        "    mini_epoch_loss = 0.0\n",
        "    epoch_loss = 0.0\n",
        "    for epoch in range(50):  # loop over the dataset multiple times  \n",
        "\n",
        "        for i, data in enumerate(train_dataloader, 0):\n",
        "            # get the inputs; data is a list of [inputs, labels]\n",
        "            inputs, labels = data\n",
        "            # print(inputs.shape)\n",
        "\n",
        "            inputs = inputs.squeeze()\n",
        "            inputs = np.moveaxis(inputs.detach().cpu().numpy(), 0, -1)\n",
        "            # print(inputs.shape)\n",
        "            preprocessed_inputs = cv.cvtColor(inputs, cv.COLOR_BGR2RGB)\n",
        "            preprocessed_inputs = Image.fromarray(np.uint8(preprocessed_inputs))\n",
        "            preprocessed_inputs = preprocessed_inputs.convert(\"RGB\")\n",
        "            preprocessed_inputs = preprocess(preprocessed_inputs)\n",
        "            preprocessed_inputs = preprocessed_inputs.unsqueeze(0)\n",
        "\n",
        "            # move the input and model to GPU for speed if available\n",
        "            if torch.cuda.is_available():\n",
        "                preprocessed_inputs = preprocessed_inputs.to('cuda')\n",
        "                labels = labels.to('cuda')\n",
        "                model.to('cuda')\n",
        "                \n",
        "\n",
        "            # zero the parameter gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # forward + backward + optimize\n",
        "            # output = model(preprocessed_inputs)['out'][0]\n",
        "            output = model(preprocessed_inputs)['aux'][0]\n",
        "\n",
        "            output = nn.Softmax(dim=0)(output)\n",
        "            output = output[1] - output[0]\n",
        "            output = torch.unsqueeze(output, 0)\n",
        "            output = torch.unsqueeze(output, 0)\n",
        "            labels = torch.reshape(labels, (1, img_height,img_width))\n",
        "            labels[labels<=0] = -1\n",
        "            labels[labels>0] = 1\n",
        "            labels = labels.long()\n",
        "\n",
        "            loss = criterion(output, labels)\n",
        "            \n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            scheduler.step()\n",
        "\n",
        "            # print statistics\n",
        "            epoch_loss += loss.item()\n",
        "            mini_epoch_loss += loss.item()\n",
        "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "            # if i % 100 == 1:    # print every 100 mini-batches\n",
        "            # if i % 2 == 1:    # print every 2 mini-batches\n",
        "                print(i)\n",
        "                print(\"cost\", loss)\n",
        "                                  \n",
        "                writer.add_scalar('average mini-epoch loss',\n",
        "                    mini_epoch_loss / 1000,\n",
        "                    epoch * len(train_dataloader) + i)\n",
        "\n",
        "                mini_epoch_loss = 0.0\n",
        "                writer.add_scalar('most recent cost',\n",
        "                                loss.item(),\n",
        "                                epoch * len(train_dataloader) + i)\n",
        "\n",
        "                # ...log a Matplotlib Figure showing the model's predictions on a\n",
        "                # random mini-batch\n",
        "                writer.add_figure('input vs output vs label',\n",
        "                                plot_classes_preds(inputs, output, labels),\n",
        "                                global_step=epoch * len(train_dataloader) + i) \n",
        "\n",
        "        writer.add_scalar('total epoch loss',\n",
        "                        epoch_loss,\n",
        "                        epoch * len(train_dataloader) + i)\n",
        "        writer.add_scalar('average epoch loss',\n",
        "                        epoch_loss / \n",
        "                        (epoch * len(train_dataloader) + i),\n",
        "                        epoch * len(train_dataloader) + i)\n",
        "\n",
        "        best_val_loss = 0\n",
        "        # Validation\n",
        "        with open(\"model_saves\\\\best_val_loss.txt\", 'r') as best_val_loss_fh:\n",
        "            lines = best_val_loss_fh.read().splitlines()\n",
        "            last_line = lines[-1]\n",
        "            best_val_loss = int(last_line)\n",
        "\n",
        "        val_loss = 0.0\n",
        "        val_steps = 0\n",
        "        for i, data in enumerate(test_dataloader, 0):\n",
        "            with torch.no_grad():\n",
        "                inputs, labels = data\n",
        "                inputs = inputs.squeeze()\n",
        "                inputs = np.moveaxis(inputs.detach().cpu().numpy(), 0, -1)\n",
        "                preprocessed_inputs = cv.cvtColor(inputs, cv.COLOR_BGR2RGB)\n",
        "                preprocessed_inputs = Image.fromarray(np.uint8(preprocessed_inputs))\n",
        "                preprocessed_inputs = preprocessed_inputs.convert(\"RGB\")\n",
        "                preprocessed_inputs = preprocess(preprocessed_inputs)\n",
        "                preprocessed_inputs = preprocessed_inputs.unsqueeze(0)\n",
        "\n",
        "                # move the input and model to GPU for speed if available\n",
        "                if torch.cuda.is_available():\n",
        "                    preprocessed_inputs = preprocessed_inputs.to('cuda')\n",
        "                    labels = labels.to('cuda')\n",
        "\n",
        "                # forward + backward + optimize\n",
        "                output = model(preprocessed_inputs)['aux'][0]\n",
        "                output = nn.Softmax(dim=0)(output)\n",
        "                output = output[1] - output[0]\n",
        "                output = torch.unsqueeze(output, 0)\n",
        "                output = torch.unsqueeze(output, 0)\n",
        "                labels = torch.reshape(labels, (1, img_height,img_width))\n",
        "                labels[labels<=0] = -1\n",
        "                labels[labels>0] = 1\n",
        "                labels = labels.long()\n",
        "\n",
        "                loss = criterion(output, labels)\n",
        "                val_loss += loss.cpu().numpy()    \n",
        "                val_steps += 1\n",
        "\n",
        "            if i % 1000 == 999:    # print every 1000 mini-batches\n",
        "                print(i)\n",
        "                print(\"val loss\", val_loss/val_steps)\n",
        "                writer.add_scalar('average mini-epoch validation loss',\n",
        "                    val_loss/val_steps,\n",
        "                    val_steps)\n",
        "\n",
        "        writer.add_scalar('average validation loss',\n",
        "                val_loss / val_steps,\n",
        "                epoch)\n",
        "\n",
        "        if val_loss / val_steps < best_val_loss:\n",
        "            print(\"New best model by validation loss!\")\n",
        "            torch.save({\n",
        "                'epoch': epoch,\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'loss': loss,\n",
        "                }, 'model_saves/'+experiment_name+\"_\"+str(int(best_val_loss))+\".pytorch_model\")\n",
        "            best_val_loss = val_loss / val_steps\n",
        "            with open(\"model_saves\\\\best_val_loss.txt\", 'a') as best_val_loss_fh:\n",
        "                best_val_loss_fh.write(str(int(best_val_loss))+\"\\n\")\n",
        "            with open(\"model_saves\\\\best_model.txt\", 'a') as best_val_loss_fh:\n",
        "                best_val_loss_fh.write('model_saves/'+experiment_name+\"_\"+str(int(best_val_loss))+\".pytorch_model\\n\")\n",
        "\n",
        "        \n",
        "\n",
        "\n",
        "\n",
        "    print('Finished Training')\n",
        "    avg_epoch_loss = epoch_loss / (epoch * len(train_dataloader) + i)\n",
        "    print(\"Average epoch loss of trial:\", avg_epoch_loss)\n",
        "\n",
        "    return avg_epoch_loss #average epoch loss\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "25f278d4",
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "999\n",
            "cost tensor(344962.3125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "1999\n",
            "cost tensor(327749.5625, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "2999\n",
            "cost tensor(327533.7188, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "3999\n",
            "cost tensor(327724.2812, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "4999\n",
            "cost tensor(326742.0938, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "5999\n",
            "cost tensor(328229.4688, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "6999\n",
            "cost tensor(329012.0938, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "7999\n",
            "cost tensor(328526.1875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "8999\n",
            "cost tensor(327989.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "9999\n",
            "cost tensor(329728.1875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "10999\n",
            "cost tensor(339534.6562, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "11999\n",
            "cost tensor(327367.9062, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "12999\n",
            "cost tensor(326793.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "13999\n",
            "cost tensor(327206.8125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "14999\n",
            "cost tensor(326781.2500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "15999\n",
            "cost tensor(340287.4375, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "16999\n",
            "cost tensor(327599., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "17999\n",
            "cost tensor(327153.5312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "18999\n",
            "cost tensor(327961., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "19999\n",
            "cost tensor(327042.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "20999\n",
            "cost tensor(327850.5938, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "21999\n",
            "cost tensor(328122.2188, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "22999\n",
            "cost tensor(327024.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "23999\n",
            "cost tensor(328653.6875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "24999\n",
            "cost tensor(326259.5312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "25999\n",
            "cost tensor(327335.6562, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "26999\n",
            "cost tensor(327019.1250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "27999\n",
            "cost tensor(327481.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "28999\n",
            "cost tensor(326596.5312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "29999\n",
            "cost tensor(325980.8125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "30999\n",
            "cost tensor(326832.2500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "999\n",
            "val loss 328441.05959375\n",
            "1999\n",
            "val loss 328554.3859375\n",
            "2999\n",
            "val loss 328495.73579166667\n",
            "3999\n",
            "val loss 328526.9156015625\n",
            "4999\n",
            "val loss 328538.69796875\n",
            "5999\n",
            "val loss 328508.86847916665\n",
            "6999\n",
            "val loss 328530.06866964285\n",
            "7999\n",
            "val loss 328545.0254882813\n",
            "8999\n",
            "val loss 328553.9529236111\n",
            "999\n",
            "cost tensor(327284.3125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "1999\n",
            "cost tensor(327156.2500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "2999\n",
            "cost tensor(328115.8125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "3999\n",
            "cost tensor(341921.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "4999\n",
            "cost tensor(327896., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "5999\n",
            "cost tensor(328492.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "6999\n",
            "cost tensor(327896.6875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "7999\n",
            "cost tensor(328131.1875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "8999\n",
            "cost tensor(327544.6875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "9999\n",
            "cost tensor(326095.7188, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "10999\n",
            "cost tensor(326710.4688, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "11999\n",
            "cost tensor(334016.5312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "12999\n",
            "cost tensor(328184.2812, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "13999\n",
            "cost tensor(327329.9062, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "14999\n",
            "cost tensor(326827.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "15999\n",
            "cost tensor(341494.5625, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "16999\n",
            "cost tensor(327302.1562, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "17999\n",
            "cost tensor(326694.1250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "18999\n",
            "cost tensor(326370., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "19999\n",
            "cost tensor(326309.1250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "20999\n",
            "cost tensor(327044.7500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "21999\n",
            "cost tensor(326920.4375, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "22999\n",
            "cost tensor(326512.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "23999\n",
            "cost tensor(326646.6250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "24999\n",
            "cost tensor(325966.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "25999\n",
            "cost tensor(326707.1875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "26999\n",
            "cost tensor(326131.2500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "27999\n",
            "cost tensor(326130., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "28999\n",
            "cost tensor(327256.0625, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "29999\n",
            "cost tensor(327357.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "30999\n",
            "cost tensor(327387.7500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "999\n",
            "val loss 328545.59934375\n",
            "1999\n",
            "val loss 328496.956421875\n",
            "2999\n",
            "val loss 328558.13944791665\n",
            "3999\n",
            "val loss 328543.9774765625\n",
            "4999\n",
            "val loss 328549.1555375\n",
            "5999\n",
            "val loss 328507.87074479164\n",
            "6999\n",
            "val loss 328511.2876071429\n",
            "7999\n",
            "val loss 328509.8782617187\n",
            "8999\n",
            "val loss 328504.40478125\n",
            "999\n",
            "cost tensor(326104.5625, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "1999\n",
            "cost tensor(325976.7500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "2999\n",
            "cost tensor(327048.9375, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "3999\n",
            "cost tensor(326898.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "4999\n",
            "cost tensor(326553.9688, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "5999\n",
            "cost tensor(326634.3125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "6999\n",
            "cost tensor(327476.7500, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "7999\n",
            "cost tensor(326774.1562, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "8999\n",
            "cost tensor(326904.3125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "9999\n",
            "cost tensor(327345.9062, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "10999\n",
            "cost tensor(326513.1250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "11999\n",
            "cost tensor(329169.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "12999\n",
            "cost tensor(333496.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "13999\n",
            "cost tensor(327549.3438, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "14999\n",
            "cost tensor(327294.3125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "15999\n",
            "cost tensor(326050.6875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "16999\n",
            "cost tensor(326539.9688, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "17999\n",
            "cost tensor(327182.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "18999\n",
            "cost tensor(340983.5312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "19999\n",
            "cost tensor(326490., device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "20999\n",
            "cost tensor(327647.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "21999\n",
            "cost tensor(326955.6250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "22999\n",
            "cost tensor(326989.6875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "23999\n",
            "cost tensor(326043.0312, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "24999\n",
            "cost tensor(326578.9375, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "25999\n",
            "cost tensor(327288.1250, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "26999\n",
            "cost tensor(327867.1562, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "27999\n",
            "cost tensor(327570.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "28999\n",
            "cost tensor(327797.8750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "29999\n",
            "cost tensor(326793.9375, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "30999\n",
            "cost tensor(336437.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "999\n",
            "val loss 328783.44921875\n",
            "1999\n",
            "val loss 328680.30865625\n",
            "2999\n",
            "val loss 328702.9063229167\n",
            "3999\n",
            "val loss 328643.4574296875\n",
            "4999\n",
            "val loss 328623.5825\n",
            "5999\n",
            "val loss 328587.51729166665\n",
            "6999\n",
            "val loss 328585.6107633929\n",
            "7999\n",
            "val loss 328552.8519140625\n",
            "8999\n",
            "val loss 328544.98439236113\n",
            "New best model by validation loss!\n",
            "999\n",
            "cost tensor(326664.8125, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "1999\n",
            "cost tensor(327261.1875, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "2999\n",
            "cost tensor(326306.5000, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n",
            "3999\n",
            "cost tensor(328242.3750, device='cuda:0', grad_fn=<SoftMarginLossBackward0>)\n"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32m<ipython-input-16-88c3e5660177>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      2\u001b[0m \u001b[1;31m# study.optimize(objective, n_trials=5)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 4\u001b[1;33m \u001b[0mobjective\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mNone\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;32m<ipython-input-15-3bd4fa39191c>\u001b[0m in \u001b[0;36mobjective\u001b[1;34m(trial, model)\u001b[0m\n\u001b[0;32m     88\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     89\u001b[0m             \u001b[1;31m# print statistics\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 90\u001b[1;33m             \u001b[0mepoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     91\u001b[0m             \u001b[0mmini_epoch_loss\u001b[0m \u001b[1;33m+=\u001b[0m \u001b[0mloss\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     92\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0mi\u001b[0m \u001b[1;33m%\u001b[0m \u001b[1;36m1000\u001b[0m \u001b[1;33m==\u001b[0m \u001b[1;36m999\u001b[0m\u001b[1;33m:\u001b[0m    \u001b[1;31m# print every 1000 mini-batches\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# study = optuna.create_study(direction='minimize', study_name=experiment_name)\n",
        "# study.optimize(objective, n_trials=5)\n",
        "\n",
        "objective(None, model)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "5f47d3d7",
      "metadata": {},
      "outputs": [],
      "source": [
        "    pruned_trials = study.get_trials(deepcopy=False, states=[TrialState.PRUNED])\n",
        "    complete_trials = study.get_trials(deepcopy=False, states=[TrialState.COMPLETE])\n",
        "\n",
        "    print(\"Study statistics: \")\n",
        "    print(\"  Number of finished trials: \", len(study.trials))\n",
        "    print(\"  Number of pruned trials: \", len(pruned_trials))\n",
        "    print(\"  Number of complete trials: \", len(complete_trials))\n",
        "\n",
        "    print(\"Best trial:\")\n",
        "    trial = study.best_trial\n",
        "\n",
        "    print(\"  Value: \", trial.value)\n",
        "\n",
        "    print(\"  Params: \")\n",
        "    for key, value in trial.params.items():\n",
        "        print(\"    {}: {}\".format(key, value))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "22e90282",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "64f2f4b5",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9c3be34c",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "fa9198b1",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a8ebdd9",
      "metadata": {},
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e79758fc",
      "metadata": {},
      "outputs": [],
      "source": [
        "def test(dataloader, model, loss_fn):\n",
        "    size = len(dataloader.dataset)\n",
        "    num_batches = len(dataloader)\n",
        "    model.eval()\n",
        "    test_loss, correct = 0, 0\n",
        "    with torch.no_grad():\n",
        "        for X, y in dataloader:\n",
        "            X, y = X.to(device), y.to(device)\n",
        "            pred = model(X)\n",
        "            test_loss += loss_fn(pred, y).item()\n",
        "            correct += (pred.argmax(1) == y).type(torch.float).sum().item()\n",
        "    test_loss /= num_batches\n",
        "    correct /= size\n",
        "    print(f\"Test Error: \\n Accuracy: {(100*correct):>0.1f}%, Avg loss: {test_loss:>8f} \\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "id": "8caa42c6",
      "metadata": {},
      "source": [
        "Show a test of the newly trained (fine tuned) model below"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "33dad314",
      "metadata": {},
      "outputs": [],
      "source": [
        "model.eval()\n",
        "\n",
        "# Display image and label.\n",
        "test_features, test_labels = next(iter(test_dataloader))\n",
        "print(f\"Feature batch shape: {test_features.size()}\")\n",
        "print(f\"Labels batch shape: {test_labels.size()}\")\n",
        "input_image = test_features[0].squeeze()\n",
        "input_image = np.moveaxis(input_image.numpy(), 0, -1)\n",
        "label = test_labels[0].reshape((img_width, img_height))\n",
        "\n",
        "plt.imshow(input_image, cmap=\"gray\")\n",
        "plt.show()\n",
        "plt.imshow(label, cmap=\"gray\")\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e2f76b07",
      "metadata": {},
      "outputs": [],
      "source": [
        "# input_image = Image.open(filename)\n",
        "input_image2 = cv.cvtColor(input_image, cv.COLOR_BGR2RGB)\n",
        "input_image2 = Image.fromarray(np.uint8(input_image2))\n",
        "input_image2 = input_image2.convert(\"RGB\")\n",
        "\n",
        "preprocess = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "input_tensor = preprocess(input_image)\n",
        "input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "# move the input and model to GPU for speed if available\n",
        "if torch.cuda.is_available():\n",
        "    input_batch = input_batch.to('cuda')\n",
        "    model.to('cuda')\n",
        "\n",
        "with torch.no_grad():\n",
        "    output = model(input_batch)['out'][0] #zero refers to the batch number?\n",
        "output_predictions = output.argmax(0)\n",
        "print(output_predictions)\n",
        "print(output_predictions.shape)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9d9d37c0",
      "metadata": {},
      "outputs": [],
      "source": [
        "test(test_dataloader, model, criterion)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "ee44acb2",
      "metadata": {},
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "name": "pytorch_vision_fcn_resnet101.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.6.8"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
